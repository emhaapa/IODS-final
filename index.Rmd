---
title: "Final Assignment"
author: "Emma Haapa"
date: "3 maaliskuuta 2017"
output: 
  html_document:
    theme: sandstone
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    fig_caption: TRUE
    fig_width: 6
    fig_height: 4
    code_folding: hide
    encoding: UTF-8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Final assignment: Linear regression on Communities and Crime dataset{#anchor}

![optional caption text](stock-photo-financial-fraud-concept-145382752.jpg)

This Course diary is related to the final assignment for the purpose of the course Introduction to Open Data Science of University of Helsinki, spring 2017. The aim of this assignment is to perform linear regression on the "Communities and crime"" dataset found in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) and validate the produced model through the model validation process introduced during the course.

## The Communities and Crime data

The data is the unnormalized version of the Communities and crime data set of Michael Redmond for Computer Science in La Salle University, Philadelphia, PA, USA. It describes the different aspects of crime and socio economic surroundings containing variables that might affect the crime rate in the U.S. communities. The dataset includes socio economic data from 1990 US Census, crime data from 1995 US FBI Uniform Crime Report and law enforcement data from 1990 US Law Enforcement Management and Administrative Statistics Survey (LEMAS), available from ICPSR at University of Michigan. The number of oservations in this dataset is 1994 and there are in total 128 variables, of which one - the total percentage of violent crimes - is considered a possible target variable and 5 non-predicive variables, such as state, county, community and community name. The rest of the 122 variables are considered predictive variables, among them for example the percentage of blacks in the community, the percentage of working moms and the number of officers assigned to special drug units. 

The population and the sum of all crimes considered violent was used in calculating the goal variable, per capita violent crimes. The interesting part in this process, according to the UCI Machine learning repository's data information page, was that there was controversy as to which kind of crimes diffeent states considered violent, which in turn led to numerous missing values in the data. Nearly all cities regarded murder, robbery, and assault as violent crimes, but some cities in midwestern states did not consider it necessary to add rape to this list, hence they were not included in the final Communities and crime data set. Before omitting those cities the "rapes" column had multiple missing values and that consequently resulted in misinterpreted values for the target variable. What I'd like to add, the rapes column still seems to 

As for why I chose this particular data set out of all the ones available in UCI machine learning repository alone, there are many reasons. First of all,  I wanted to have a look on a different data to all of the sets we explored durng the course, and see what I could do on my own. Secondly, but notof inferior importance, I am very interested in crime related data in general, and the relationships of different aspects of population and socio economic variables affecting the crime rate especially in the US. This was only further accelerated by the glimpse we had on a data set called Boston (included in the MASS package of the R software) during the IODS course, however, I felt like I wanted to have a more widespread look on the crime in the US - and as I stumbled uppon the Communities and Crime data set, it felt like the perfect one to go on an explore in my final project. 

As the process of this exploration went on, I noticed it turned out to include a multitude of missing values. In order to delve into Primary Component Analysis with regard to this data, I came into the conclusion of performing na.omit on the data in the process of doing this exercise. Furthermore, I decided to choose the unnormalized data set instead of the normalized version, also available in the UCI repository, because the values in the data matrix seemed to be more easily comprehensible and comparable, whereas in the normalized data set all od the values were in the range of 0 to 1. What is more, the normalization performed on the data, I was afraid, would have left some extreme observations out. I learned that while performing linear regression, no standardization or normalization is essentially necessary, since the parameters and their signifigance is relatively the same whether performed on a standardized or an unstandardized data set. Also, a normalization on the data would be easy to perform for example with the data.Normalization()-function from the clusterSim-package in the following way:
CommunityCrime22_sn <- data.Normalization (CommunityCrime22,type="n4",normalization="column"),
where type = "n4" refers to the type of normalization - in this case "n4 - unitization with zero minimum ((x-min)/range)".


## The data wrangling process on the Communities and Crime data

The data available in the UCI repository didn't contain names for the variables at all; they were only given in the description of the data in a list with descriptions of the variables included. Accordingly, before any data wrangling could be done, I created a list of column names in a separate .txt-file where I removed all other information in the variables list found on the data information page. In the R scrip where the data wrangling was done, i then assigned the column names in the data to be taken from this columnnames.txt-data, that I put saved into a separate object.
The actual data wrangling process for this data set consisted of removing columns with numerous missing values and omitting some non-predictive variables that are not of interest in building a linear regression model (those included for example county, community and community name). In this phase, I created a data set called CommunityCrime11 that included all of the remaining variables, for my interest in fitting a linear regression model on all the variables. Furthermore, in the finalized data set I only kept some interesting variables that I hypothesized to have some kind of significant effect on the total crime variable. Those, accompanied with their descriptions found on the [data information page](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized#) in the following list, were:

* **"ViolentCrimesPerPop"** =  total number of violent crimes per 100K population
* **"population"** =  population for community
* **"PctForeignBorn"** = percent of people foreign born
* **"NumInShelters"** = number of people in homeless shelters
* **"NumStreet"** = number of homeless people counted in the street
* **"PctHomeless"** = number of homeless people in homeless shelters plus homeless in the street divided by population times 100
* **"PopDens"** = population density in persons per square mile
* **"perCapInc"** = per capita income
* **"medIncome"** =  median household income
* **"PctLess9thGrade"** = percentage of people 25 and over with less than a 9th grade education
* **"PctRecentImmig"** = percent of population who have immigrated within the last 3 years
* **"racepctblack"** = percentage of population that is african american
* **"PctEmploy"** = percentage of people 16 and over who are employed
* **"MalePctNevMarr"** = percentage of males who have never married 
* **"PctWorkMom"** = percentage of moms of kids under 18 in labor force
* **"PctPopUnderPov"** = percentage of people under the poverty level 
* **"PctPersDenseHous"** = percent of persons in dense housing (more than 1 person per room) 
* **"PctUsePubTrans"** = percent of people using public transit for commuting 

As a side not, among all other interesting variables not included in the final data set, there was also a variable in the original data called **"RacialMatchCommPol"** = a measure of the racial match between the community and the police force, high values indicate proportions in community and police force are similar, which initially seemed quite interesting in terms of analysis. However, I decided not to include it in my wrangled data set as it contained a high number of missing values and in the end didn't seem to provide any information for some of the communities included in the data.

I also wanted the row names of this data to be distinctified by the community names followed by the state column's character string indicating in which state this community is in, so I assigned the row names to be taken from those two columns and subsequently removed them from the dataset. The more tidy and comprehensible data set resulting from this data wrangling I called CommunityCrime22.


## Looking into the data: plots and tables

First off, let's have a peek at the latter, smaller data with the variables mentioned in the preceding list through tables and plots.

```{r, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
# Accessing libraries:
library(gmodels)
library(gdata)
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr) 
library(tidyverse)
library(corrplot)
library(GGally)
library(stringr)
options(max.print=1000000)

CommunityCrime22 <- read.table('CommunityCrime22.txt', sep=",", header = TRUE)
rownames(CommunityCrime22)
str(CommunityCrime22)
glimpse(CommunityCrime22)

```

As we can see from the initial exploration done with R's rownames()- and glimpse()-functions, the row names actually do correspond with the communities in question and we can also have a look at which kind of values the included variables get: mostly numerical values. Let's now look at the summary of the data:

```{r, message=FALSE, warning=FALSE, cache=TRUE}
summary(CommunityCrime22)
```


We can first spot that only the target variable ViolentCrimesPerPop includes missing values, 221 of them in total. This, among other reasons, is due to the missing values in rapes-column, which was whopping looking at the separate rapes-column in the original data set. Moreover, the values of violent crimes per population range from zero to almost 5 000 with a median at around 374, which tells us that there is variation between cities: some of them have a high rate of violent crimes whereas some of the cities don't seem to have violent crimes at all (minimum at zero). 
From the summary of the population column it can be concluded that the data included small communities with as little as 10 005 in population, and also very large communities with a high number of people going up to over 7,3 million. This also affects the population density, which varies from 10 persons per square mile to a staggering 44 230 persons per square mile. What is interesting in the percent of foreign born people -column, is that it varies significantly: some communities have only 0,18 percent of foreign born recidents whereas the maximum is as high as 60,4 percent - more than a half of the people are foreign born in some communities. The same goes for racepctblack, the percent of people of African American heritage: the minimum is a straight zero, the median and mean are both quite low, but the maximum is 96,67 (!), extremely high.

The next interesting columns are the number of homeless people in shelters - NumInShelters - and number of homeless people found in the streets - NumStreet - columns. We can see that the median for both is 0,00 and the mean for the first is 66,95 and for the latter 17,82. This tells about a significantly uneven distribution of homeless people in the communities - some communities have a large amount of homeless people, and most have none. The maximum values are high compared to the mean value: for homeless in shelters it is 23 383 and for homeless in the streets 10 447. This may be due to some communities being more urban and more populated, whereas there are many small, "countryside" communities (for example from the midwestern USA) in the data that probably have a smaller number of homeless people around (and people in general, for that matter).

The public transport and people in dense household columns also seem to present a rather skewed distribution or distributions containing outliers based on the median, mean and maximum values; for the public transport the median is 1,2 when the maximum is 54,33 and for dense households the median 2,3, the mean 4,1 and the maximum over 59 percent. This may be a result of the same reason for why the homeless people distribute so unevenly: small townships have little to no public transport, when in turn people in bigger, more populated cities use public transport that is available from every corner, going to every place in the neighbourhood and moreover, people have to be more densely habitated in bigger cities where the population density is high. It can also be speculated that the reason for people in dense households may be due to people with low income, big families or different culture (foreign born citizens) which in turn might be in connection with each other and result in more people residing under the same roof. This, of course, is merely speculation.

The income related columns - medIncome, perCapInc and PctPopUnderPov - are also worth exploring. The latter presents a wide range of values with a low minimum and a high maximum. The median and mean are quite close to each other but rather far from the maximum value: generally the percent of people under the poverty rate is close to 10 percent, whereas in some cities over half of the people are under the poverty rate. The per capita income variable ranges from approximately 5 000 to 65 000, which is quite a wide range, peaking, however, at around 15 000. So most of the cities has a 15 thousand income per capita, but there are exceptionally high values as well as some low values.
Median income has an even wider range of approximately 9 000 to some 122 000, wen the mean and median are around 30 000 - so some outlier communities are definitely present in the data.

Let's look at the distributions and correlations more visually with ggpairs() and corrplot():
```{r, fig.height=25, fig.width=15, message=FALSE, warning=FALSE, cache=TRUE}
library(GGally)

ggpairs(CommunityCrime22, mapping = aes(), lower = list(combo = wrap("facethist", bins = 10)), upper = list(continuous = wrap("cor", size=3)))
```


```{r, message=FALSE, warning=FALSE, cache=TRUE}
CommunityCrime2211 <- na.omit(CommunityCrime22)
summary(CommunityCrime2211)

corrplot(round(cor(CommunityCrime2211), digits = 2), method = "circle", type = "upper", tl.pos = "d", tl.cex = 0.8)
```

We can straight off see from the plots, that there isn't very much brightly standing out correlation with the variables in this data set with the supposed target variable, violent crimes per population. There is some slight correlation that can be detected from the latter corrplot, however, the variables correlate more strongly with each other than they do with violent crimes. The correlations are, however, the most obvious ones: for example the number of homeless people in the streets correlates positively with number of homeless in shelters, the latter two of which correlate positively with population, which somewhat confirms my initial ponderings of these being in connection. What is more, percent of foreign born correlates positively with percent of recently immigrated of population. Also percent of peopl eunder poverty rate correlates negatively with median income. Some of my earlier ponderings about foreign born people and dense housing being in connection turns out to be true looking at the corrplot-table: they seem to be in slight positive correlation.
Since the purpose of this assignment is merely to exercise the abilities gained during this course, not necessarily to find interesting and eye-popping results, we can continue with this line of analysis despite the ggpairs- and corrplot presenting quite disillusioning results. 

## PCA on the Community and Crime data

Next, to reduce the dimensionality of this data set, I'll move on to perform Primary Component Analysis (PCA) on it. A Principal Component Analysis (abbreviated as PCA) can be conducted on the dataset to lower its dimensionality using the singular value decomposition (SVD) method. The SVD literally decomposes a matrix and produces a product of smaller matrices, which brings out the most important components, and in statistics, PCA aims to do exactly that. It represents the data at hand in two dimensions with respect to Principal components - the components which explain different proportions of variance of the original variables, or in other words, features. The first Principal component always explains the largest proportion of variance along the features, and the importance of the PC is relative to the order in which it is represented in the PCA: for example, PC2 explains the second largest proportion of variance and PC3 the third largest and so on. Usually only two principal components explain the most significant amount of variance, hece the two-dimensionality of the PCA and the resulting biplot (bi = two) with arrows representing the correlation between the variables with each other and with the two PCs. Consequently, the Principal components are also uncorrelated, which means that the angle between them is 90 degrees (which corresponds to a 0 correlation).

To do that we need to convert the data set into a data frame and scale it - otherwise the skewed distributions of the variables and the differing variances would lead PCA to "perceive" the variables with larger variance as also more important, because PCA is impacted by the scaling of the features.
  
    


```{r, message=FALSE, warning=FALSE, cache=TRUE}
library(clusterSim)
CommunityCrime22111_s <- as.data.frame(scale(CommunityCrime2211))

summary(CommunityCrime22111_s)
pcaCC2211_std <- prcomp(CommunityCrime22111_s) # Perform the PCA.
sumpcaCC2211_std <- summary(pcaCC2211_std) # Summarise the results.
sumpcaCC2211_std # Print summary.

```

As we can see from the previous table, the Proportions of variance for the two first primary components (0,2793 and 0,1998) do not look very promising - essentially, the two first primary components explain less than half of the total variance - but let's continue on to drawing the biplot of this result.


```{r, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, cache=TRUE}
# Extract percentages of variance from the summary (for the plot titles).
pr_CC2211s <- round(100*sumpcaCC2211_std$importance[2, ], digits = 1)
pc_CC2211_lab <- paste0(names(pr_CC2211s), " (", pr_CC2211s, "%)") # Create plot axis titles.
# Plot
biplot(pcaCC2211_std, cex = c(0.8, 1), col = c("LightGray", "black"), xlab = pc_CC2211_lab[1], ylab = pc_CC2211_lab[2], xlim=c(-0.05, 0.11), ylim=c(-0.05, 0.1))
```



With the initial settings the plot wasn't exactly readable; because of the quite remarkable outliers (New York, for example) that made the scale of the x- and y-axii very wide. That's why I used xlim and ylim to limit the range of the axii and produce a more readable biplot. Now, from the above plot we can see from the direction of the arrows, which features contribute to either of the primary components. Clearly, percent of people in dense housing, percent of male persons never married, percent of homeless as well as violent crimes per population (our target variable for linear regression) percent of people with lower than or 9th grade education and percent of African Amrican of the population correlate with the first principal component. Also percent of working moms with under 18 year old kids seems to fall in this category, however, the arrow points to the opposite direction. Features that contribute to both of the first primary components but might contribute more to the PC2, are percent of people under poverty rate, median and per capita income and percent of employed people, which all have to do with income related aspects. These include also percent of people using public transport, percent of recently immigrated of population, number of homeless in the streets and in shelters, population and population density - population related features which also have to do with income. Furthermore, the angle between a single feature and the PC-axis can be interpreted as the correlation between these two, a small angle indicating strong positive correlation. The arrows which are in a close to 90 degrees angle with either PC1 or PC2 have close to a 0 correlation with these PCs, so only for percent of people in dense housing, percent of male persons never married, percent of homeless as well as violent crimes per population we can say for sure they are in almost no correlation with PC2, having only a small angle with PC1. 

From the biplot we can also read the correlations _between_ the variables. For example, percent of people in dense housing, percent of male persons never married and percent of homeless seem to be in positive correlation with each other since the angle between them is nonexistent. There is also positive correlation between violent crime per population and the aforementioned features and percent of people with lower than or 9th grade education and percent of African Amrican of the population as well as percent of people under poverty rate seem to be in positive correlation with the target variable. The target variable and the income variables described plus the percent of employed people variable are in negative correlation: the corresponding arrows point to opposite directions.


## Thoughts and hypotheses
 
From the Primary Component Analysis conducted in the previous passage, as well as from my own initial hypotheses, I concluded that I could attempt to fit the linear model based on the following variables: 


* **"ViolentCrimesPerPop"** =  total number of violent crimes per 100K population (as the target variable, to be explained)
* **"PctHomeless"** = number of homeless people in homeless shelters plus homeless in the street divided by population times 100
* **"perCapInc"** = per capita income
* **"PctLess9thGrade"** = percentage of people 25 and over with less than a 9th grade education
* **"racepctblack"** = percentage of population that is african american
* **"PctEmploy"** = percentage of people 16 and over who are employed
* **"MalePctNevMarr"** = percentage of males who have never married 
* **"PctWorkMom"** = percentage of moms of kids under 18 in labor force
* **"PctPopUnderPov"** = percentage of people under the poverty level 
* **"PctPersDenseHous"** = percent of persons in dense housing (more than 1 person per room) 

My hypothesis at this point is that all of the above variables have a significant linear relationship with the target variable.


## Linear regression on the dataset

Linear regression provides a means to create simple statistical models that attempt to explain the relationship between a target variable and one or more explanatory variables, called simple regression and multiple regression, respectively. The linear model explains only a linear relationship between the variables, not any other kind of relationship or correlation, which encourages me to attempt to fit a model on this data despite the low correlations. The idea in the interpretation of the model is to look at the parameters - the intercept, which is called the alpha-parameter, and the "slope" which is called the beta-parameter and can be interpreted as the coefficient of the linear dependency between the explanatory variable and the target variable. 

The validation of the soon produced model tackles the assumptions, on which the model is based - these include for example the normality and constant variance of the errors. The linear model in R can be fitted with lm()-function and the parameters of the model can be read from the summary-table produced by the regular summary-function:

```{r, message=FALSE, warning=FALSE, cache=TRUE}

library(ggplot2)
library(GGally)

# create a regression model with multiple explanatory variables
my_model4 <- lm(ViolentCrimesPerPop ~ racepctblack + PctHomeless + PctEmploy + PctWorkMom + PctPersDenseHous + PctPopUnderPov + perCapInc + MalePctNevMarr+  PctLess9thGrade, data = CommunityCrime22)

# print out a summary of the model
summary(my_model4)

```

We can see from the summary-table that this initial approach didn't produce parameters corresponding to the variables that would all statistically significantly differ from zero. We can see that from the p-value column of the summary table: the statistical test produced p-values that aren't significant on any common significancy level, represented by * (stars) in the end of the rows. The statistical test related to the parameters is namely a test for the null-hypothesis that the beta-parameter in the regression model is actually zero. The p-value indicates the probability of that happening, using this statistical model. The influence of an explanatory variable is statistically significant, if it (in general) is <0,05. The highest value the p-value could take to be statistically at least somewhat significant, is 0,1.

In the above summary-table, we can see that there are three variables, percent of people under poverty rate, per capita income and percent of male persons never married, for which the statistical test for the beta-parameter produced p-values that were under the generally used significancy levels.We can try to fit the model without these variables and see where it takes us, or remove a variable at a time. I tried both approaches, and it resulted in two equally nice models, one where I removed PctPopUnderPov + perCapInc + MalePctNevMarr+ +  PctLess9thGrade (first I removed all of the insignificant ones and was left with another insignificant variable, PctLess9thGrade, which I subsequently removed) and one where I removed PctLess9thGrade, MalePctNevMarr and PerCapInc, one at a time, which indicates that the percent of population under pverty rate doesn't influence the model in a significant way. Let's print the summary of the first model.


```{r, message=FALSE, warning=FALSE}
# create a regression model with multiple explanatory variables
CC22_lm <- lm(ViolentCrimesPerPop ~ racepctblack + PctHomeless + PctEmploy + PctWorkMom + PctPersDenseHous, data = CommunityCrime22)

# print out a summary of the model
summary(my_model4)
```

```{r, message=FALSE, warning=FALSE}
# create a regression model with multiple explanatory variables
CC22_lm1 <- lm(ViolentCrimesPerPop ~ racepctblack + NumStreet+ PctWorkMom + MalePctNevMarr + perCapInc, data = CommunityCrime22)
 # tai + PctEmploy 
# print out a summary of the model
summary(CC22_lm1)
```


KORJAA TÄÄ:
reading from the table, the first estimate, i.e. the beta-parameter, is 3,5255 which indicates a strong positive influence on points. So a good attitude correlates with good points, simply said. The p-value for deep is 0.897, which is not at all a good p-value - it is significantly greater than 0,1, so the influence of deep on points cannot be considered statistically significant. For stra, the p-value is 0.0603, which is pretty good - it is close to 0,05 and definitely less than 0,1.
The summary-table shows the beta-parameters for each of the aforementioned variables, and also that the p-value for 'attitude' is 0,0000000193 - which is by all means strictly less than 0,05! Now, with this model, at least for the attitude parameter, the probability of making a Type I -error (discarding error) is very little. For the other parameters, stra and surf, both have >0,1 p-values, which gives the results signifigance level to be quite low and the probability of making the Type I -error (of discaring the null hypothesis) slightly greater.
The model parameters can be read from the summary-table. The alpha-parameter indicates, where the regression line intercepts the y-axis. In this case it is 8,9729. The beta-parameter for attitude in this model is 3,4658, which indicates a strong positive influence of attitude on exam points, i. e. the explanatory and target variable. The beta-parameter can be viewed as the slope of the curve which indicates the influence of this variable on the target variable; in this case the line is ascending. In the case of the stra-variable, the beta-parameter is a little bit smaller in value, but still positive, so it can be said that the strategic learning style has a positive influence on exam points - only a bit lesser of an influence than the attitude-variable has.

**R-squared** is a statistical measure of how close the data are to the fitted regression line. It is also known as 'the coefficient of determination', or 'the coefficient of multiple determination' for multiple regression, as in this case. By definition, R-squared is the percentage of the response variable variation that is explained by a linear model. In other words: how well the model explains the main variables variation, as opposed to other factors that might affect the main variable. It can be written as

R-squared = Explained variation / total variation

R-squared is always between 0 and 100%: If R-squared is 0% that indicates the model explains none of the variability of the response data around its mean. If R-squared is 100% that indicates that the model explains all the variability of the response data around its mean.

In general, the higher the R-squared, the better. Taking into consideration this notion, the multiple R-squared is 0,5538 and the Adjusted R-squared is 0,5527 (not remarkably lower, which indicated that the moel's variables explain (eti se sas tiedosto) adding a random variable in the model doesn't )


## Model validation

In this chapter we will graphically validate the model.

```{r, message=FALSE, warning=FALSE}

plot(CC22_lm1, which = c(2))

```


In linear regression the assumptions include that the target variable is assumed to be a linear combination of the explanatory variables - i. e. the fisrt big assumption associated with a linear model is linearity. Generally, it is useful to assume also, that the errors are normally distributed and moreover, with a mean of 0 and a constant variance. This includes also the notion that the errors are not correlated and that the size of an error does not depend on the explanatory variables. There are a number of graphical ways to explore these assumptions, for example:

**The Residuals vs. Fitted values -plot**
The Residuals vs. Fitted values -plot explores the constant variance assumption of the errors in the model, in other words the assumption that there should be no or close to none dependency between the errors and the explanatory variables. The simple plot shows residuals and model predictions, and if there seems to be any pattern in the plot, one could conclude that there is some dependency and the variance of the errors is not constant.

In the plot of the model, the points seem to be distributed quite randomly, and no actual pattern can really be seen. The curve fitted to these points is also rather vertical, with no deviation or slope differing from 1, so it is a good sign that there is no dependency of the errors on the explanatory variables. In this case, I would say that the variance seems to be constant, and this assumption of the linear model holds true.

**The Normal QQ-plot**
The Normal QQ-plot is a plot of the residuals that can be used to explore the assumption of the normality of the errors in the model. In the Normal QQ-plot of the model in question, the points of the plot seem to fall pretty nicely on the curve, and nothing seems to deviate out of the line. Consequently, according to this graphical model validation plot, the model's errors are (at least close to being) normally distributed and the normality assumption is correct.

**The Residual vs. Leverage -plot**

The Residual vs. Leverage -plot measures, how much impact a single observation has on the model. By analyzing the plot, any observations that have a high impact on the model, can be identified. The model of the two explanatory variables affecting the points-variable, we can see that the scale of the Leverage-plot is not very wide - only from 0 to 0,05. This indicates, that no observation is stranded outside the plot, and if there are observations in the fringes of the plot, the distance to other observations is not so big. Also, all the observations seem to fall in the plot quite evenly, which in itself indicates that there are no single observations having unusually high impact on the model. So the linearity assumption can be said to be correct in this model.




## Conclusions


