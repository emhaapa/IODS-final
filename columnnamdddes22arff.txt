Furthermore, the angle between a single feature and the PC-axis can be interpreted as the correlation between these two, a small angle indicating strong positive correlation. This is very visual and makes the analysis part interesting: we can see from the plot that nearly all of the variables in connection with PC1 have relatively small angles and thus strong positive connection with PC1. Those ariables contributing to PC2 also have a small angle with the PC2-axis, so the correlation between the PC2 and them is also significantly positive. The arrows which are in a close to 90 degrees angle with either PC1 or PC2 have close to a 0 correlation with these PCs, so indeed for example labRatioFM doesn’t correlate with PC1 - which is an element of PCA - the axii are uncorrelated. 



Next, to reduce the dimensionality of this data set, I'll move on to perform Primary Component Analysis (PCA) on it. A Principal Component Analysis (abbreviated as PCA) can be conducted on the dataset to lower its dimensionality using the singular value decomposition (SVD) method. The SVD literally decomposes a matrix and produces a product of smaller matrices, which brings out the most important components, and in statistics, PCA aims to do exactly that. It represents the data at hand in two dimensions with respect to Principal components - the components which explain different proportions of variance of the original variables, or in other words, features. The first Principal component always explains the largest proportion of variance along the features, and the importance of the PC is relative to the order in which it is represented in the PCA: for example, PC2 explains the second largest proportion of variance and PC3 the third largest and so on. Usually only two principal components explain the most significant amount of variance, hece the two-dimensionality of the PCA and the resulting biplot (bi = two) with arrows representing the correlation between the variables with each other and with the two PCs. Consequently, the Principal components are also uncorrelated, which means that the angle between them is 90 degrees (which corresponds to a 0 correlation).

To do that we need to convert the data set into a data frame and scale it - otherwise the skewed distributions of the variables and the differing variances would lead PCA to "perceive" the variables with larger variance as also more important, because PCA is impacted by the scaling of the features.



With the initial settings the plot wasn't exactly readable; because of the quite remarkable outliers (New York, for example) that made the scale of the x- and y-axii very wide. That's why I used xlim and ylim to limit the range of the axii and produce a more readable biplot. Now, from the above plot we can see from the direction of the arrows, which features contribute to either of the primary components. Clearly, percent of people in dense housing, percent of male persons never married, percent of homeless as well as violent crimes per population (our target variable for linear regression) percent of people with lower than or 9th grade education and percent of African Amrican of the population correlate with the first principal component. Also percent of working moms with under 18 year old kids seems to fall in this category, however, the arrow points to the opposite direction. Features that contribute to both of the first primary components but might contribute more to the PC2, are percent of people under poverty rate, median and per capita income and percent of employed people, which all have to do with income related aspects. These include also percent of people using public transport, percent of recently immigrated of population, number of homeless, population and population density - population related features which also have to do with income. Furthermore, the angle between a single feature and the PC-axis can be interpreted as the correlation between these two, a small angle indicating strong positive correlation. This is very visual and makes the analysis part interesting: we can see from the plot that nearly all of the variables in connection with PC1 have relatively small angles and thus strong positive connection with PC1. Those ariables contributing to PC2 also have a small angle with the PC2-axis, so the correlation between the PC2 and them is also significantly positive. The arrows which are in a close to 90 degrees angle with either PC1 or PC2 have close to a 0 correlation with these PCs, so indeed for example labRatioFM doesn’t correlate with PC1 - which is an element of PCA - the axii are uncorrelated. 



The statistical test related to the parameters is a test for the null-hypothesis, that the beta-parameter in the regression model is actually zero. The p-value indicates the probability of that happening, using this statistical model. The influence of an explanatory variable is statistically significant, if it (in general) is <0,05. The highest value the p-value could take to be statistically at least somewhat significant, is 0,1.

In the above summary-tables, the p-values are for attitude 4.12e-09, which is extremely good, and strictly less than 0,001 which is the best of the signifigance levels. So the influence of attitude to points seems to be significant, and reading from the table, the first estimate, i.e. the beta-parameter, is 3,5255 which indicates a strong positive influence on points. So a good attitude correlates with good points, simply said. The p-value for deep is 0.897, which is not at all a good p-value - it is significantly greater than 0,1, so the influence of deep on points cannot be considered statistically significant. For stra, the p-value is 0.0603, which is pretty good - it is close to 0,05 and definitely less than 0,1.
The summary-table shows the beta-parameters for each of the aforementioned variables, and also that the p-value for ‘attitude’ is 0,0000000193 - which is by all means strictly less than 0,05! Now, with this model, at least for the attitude parameter, the probability of making a Type I -error (discarding error) is very little. For the other parameters, stra and surf, both have >0,1 p-values, which gives the results signifigance level to be quite low and the probability of making the Type I -error (of discaring the null hypothesis) slightly greater.
The model parameters can be read from the summary-table. The alpha-parameter indicates, where the regression line intercepts the y-axis. In this case it is 8,9729. The beta-parameter for attitude in this model is 3,4658, which indicates a strong positive influence of attitude on exam points, i. e. the explanatory and target variable. The beta-parameter can be viewed as the slope of the curve which indicates the influence of this variable on the target variable; in this case the line is ascending. In the case of the stra-variable, the beta-parameter is a little bit smaller in value, but still positive, so it can be said that the strategic learning style has a positive influence on exam points - only a bit lesser of an influence than the attitude-variable has.

R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as ‘the coefficient of determination’, or ‘the coefficient of multiple determination’ for multiple regression, as in this case. By definition, R-squared is the percentage of the response variable variation that is explained by a linear model. In other words: how well the model explains the main variables variation, as opposed to other factors that might affect the main variable. It can be written as

R-squared = Explained variation / total variation

R-squared is always between 0 and 100%: If R-squared is 0% that indicates the model explains none of the variability of the response data around its mean. If R-squared is 100% that indicates that the model explains all the variability of the response data around its mean.

In general, the higher the R-squared, the better. Taking into consideration this notion, the multiple R-squared is 



In linear regression the assumptions include that the target variable is assumed to be a linear combination of the explanatory variables - i. e. the fisrt big assumption associated with a linear model is linearity. Generally, it is useful to assume also, that the errors are normally distributed and moreover, with a mean of 0 and a constant variance. This includes also the notion that the errors are not correlated and that the size of an error does not depend on the explanatory variables. There are a number of graphical ways to explore these assumptions, for example:

The Residuals vs. Fitted values -plot
The Residuals vs. Fitted values -plot explores the constant variance assumption of the errors in the model, in other words the assumption that there should be no or close to none dependency between the errors and the explanatory variables. The simple plot shows residuals and model predictions, and if there seems to be any pattern in the plot, one could conclude that there is some dependency and the variance of the errors is not constant.

In the plot of the model, the points seem to be distributed quite randomly, and no actual pattern can really be seen. The curve fitted to these points is also rather vertical, with no deviation or slope differing from 1, so it is a good sign that there is no dependency of the errors on the explanatory variables. In this case, I would say that the variance seems to be constant, and this assumption of the linear model holds true.

The Normal QQ-plot
The Normal QQ-plot is a plot of the residuals that can be used to explore the assumption of the normality of the errors in the model. In the Normal QQ-plot of the model in question, the points of the plot seem to fall pretty nicely on the curve, and nothing seems to deviate out of the line. Consequently, according to this graphical model validation plot, the model’s errors are (at least close to being) normally distributed and the normality assumption is correct.

The Residual vs. Leverage -plot
The Residual vs. Leverage -plot measures, how much impact a single observation has on the model. By analyzing the plot, any observations that have a high impact on the model, can be identified. The model of the two explanatory variables affecting the points-variable, we can see that the scale of the Leverage-plot is not very wide - only from 0 to 0,05. This indicates, that no observation is stranded outside the plot, and if there are observations in the fringes of the plot, the distance to other observations is not so big. Also, all the observations seem to fall in the plot quite evenly, which in itself indicates that there are no single observations having unusually high impact on the model. So the linearity assumption can be said to be correct in this model.


Linear model themselves are simply statistical models that try to model a relationship between a target variable – in this case, the ratio of secondary or higher education – and one or more explanatory variables. Their most important assumption is linearity, but there are also other assumptions (more of which below).


we can indeed see that the correlations between the variables are shown in the biplot. For example, the correlation between maternal mortality and adolescent birth is strongly positie, 0,76 as read from the correlation matrix, and from the biplot we can see that in between the arrows there is only a small angle. As opposed to for example expected years of education and maternal morality, which have a strong negative correlation, that of -0,74, the biplot also indicates that the corresponding arrows point to the opposite directions.